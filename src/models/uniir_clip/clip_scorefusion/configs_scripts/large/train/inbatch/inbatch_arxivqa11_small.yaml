data_config:
  enable_query_instruct: true
  hard_neg_num: 0
  image_size: 224, 224
  in_batch_neg_num: 0
  query_instruct_path: instructions/query_instructions.tsv
  returns: null
  shuffle_cand: true
  train_cand_pool_path: passage/train_11_small.jsonl
  train_query_data_path: query/train_11_small.jsonl
  val_cand_pool_path: passage/val_11_small.jsonl
  val_query_data_path: query/val_11_small.jsonl
dataloader_config:
  num_workers: 5
  train_batch_size: 105
  valid_batch_size: 1024
dist_config:
  dist_url: env://
evaluator:
  enable_eval: true
  eval_freq: 1
  print_freq: 10
experiment:
  description: ${model.name} ${model.size} ${experiment.instruct_status} ${experiment.exp_name}
  exp_name: ArxivQA11_small
  instruct_status: Instruct
  path_suffix: ${model.short_name}/${model.size}/${experiment.instruct_status}/${experiment.exp_name}/
logger_config:
  logger_out_dir: logger/${experiment.path_suffix}
  logger_out_file_name: train.log
model:
  ckpt_config:
    ckpt_dir: checkpoint/${experiment.path_suffix}
    ckpt_name: ''
    resume_training: false
  clip_vision_model_name: ViT-L/14
  gather_embeddings: true
  name: CLIPScoreFusion
  pretrained_clip_model_dir: checkpoint/CLIP/
  short_name: CLIP_SF
  size: Large
seed: 2023
trainer_config:
  eval_steps: 500
  gradient_accumulation_steps: 1
  learning_rate: 1e-5
  num_train_epochs: 20
  print_freq: 50
  warmup_steps: 0
wandb_config:
  enabled: true
  experiment_name: ${experiment.description}
